{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python Dictionaries\n",
    "\n",
    "Python is built around dictionaries. The various namespaces include globals, locals, module dictionaris, class dictionaries, instance dictionaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Dictionaries\n",
    "Create a class to track user assignments with a property category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'guido': 'blue', 'sarah': 'orange', 'barry': 'green', 'rachel': 'yellow', 'tim': 'red'}\n",
      "{'guido': 'austin', 'sarah': 'datas', 'barry': 'tuscon', 'rachel': 'reno', 'tim': 'portland'}\n",
      "{'guido': 'apple', 'sarah': 'banana', 'barry': 'orange', 'rachel': 'pear', 'tim': 'peach'}\n",
      "[112, 112, 112]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "class UserProperty:\n",
    "    def __init__(self, v0, v1, v2, v3, v4):\n",
    "        self.guido = v0\n",
    "        self.sarah = v1\n",
    "        self.barry = v2\n",
    "        self.rachel = v3\n",
    "        self.tim = v4\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return 'UserProperty(%r, %r, %r, %r, %r)' \\\n",
    "                % (self.guido, self.sarah, self.barry, self.rachel, self.tim)\n",
    "    \n",
    "colors = UserProperty('blue', 'orange', 'green', 'yellow', 'red')\n",
    "cities = UserProperty('austin', 'dallas', 'tuscon', 'reno', 'portland')\n",
    "fruits = UserProperty('apple', 'banana', 'orange', 'pear', 'peach')\n",
    "\n",
    "for user in [colors, cities, fruits]:\n",
    "    print(vars(user))\n",
    "\n",
    "print(list(map(sys.getsizeof, map(vars, [colors, cities, fruits]))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolution: A dozen good ideas\n",
    "In the beginning dinosaus roamed the earth, there were databases (columns, rows)\n",
    "\n",
    "_Index into the database_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "Here is our sample data to store in our dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "from pprint import pprint\n",
    "\n",
    "keys = 'guido sarah barry rachel tim'.split()\n",
    "values1 = 'blue orange green yelow red'.split()\n",
    "values2 = 'austin dallas tuscon reno portland'.split()\n",
    "values3 = 'apple banana orange pear peach'.split()\n",
    "hashes = list(map(abs, map(hash, keys)))\n",
    "entries = list(zip(hashes, keys, values1))\n",
    "comb_entries = list(zip(hashes, keys, values1, values2, values3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How a Database Would Do It\n",
    "The data is dense (no holes or over-allocations). Without an index, the search is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('guido', 'blue', 'austin', 'apple'),\n",
      " ('sarah', 'orange', 'dallas', 'banana'),\n",
      " ('barry', 'green', 'tuscon', 'orange'),\n",
      " ('rachel', 'yelow', 'reno', 'pear'),\n",
      " ('tim', 'red', 'portland', 'peach')]\n"
     ]
    }
   ],
   "source": [
    "def database_linear_search():\n",
    "    pprint(list(zip(keys, values1, values2, values3)))\n",
    "# Structure:    \n",
    "database_linear_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How LISP Would Do It\n",
    "stores list of pairs. (key-value tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('guido', 'blue'),\n",
      "  ('sarah', 'orange'),\n",
      "  ('barry', 'green'),\n",
      "  ('rachel', 'yelow'),\n",
      "  ('tim', 'red')],\n",
      " [('guido', 'austin'),\n",
      "  ('sarah', 'dallas'),\n",
      "  ('barry', 'tuscon'),\n",
      "  ('rachel', 'reno'),\n",
      "  ('tim', 'portland')],\n",
      " [('guido', 'apple'),\n",
      "  ('sarah', 'banana'),\n",
      "  ('barry', 'orange'),\n",
      "  ('rachel', 'pear'),\n",
      "  ('tim', 'peach')]]\n"
     ]
    }
   ],
   "source": [
    "def association_lists():\n",
    "    pprint([\n",
    "        list(zip(keys, values1)),\n",
    "        list(zip(keys, values2)),\n",
    "        list(zip(keys, values3)),\n",
    "    ])\n",
    "association_lists()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sparate Chaining\n",
    "Use multiple buckets to reduce the linear search by a constant factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(2115095632030872316, 'guido', 'blue'), (5958691992108336732, 'sarah', 'orange'), (6831221632828924534, 'barry', 'green'), (3038847795535729636, 'rachel', 'yelow')], [(9001216757337362437, 'tim', 'red')]]\n"
     ]
    }
   ],
   "source": [
    "def separate_chaining(n):\n",
    "    buckets = [[] for i in range(n)]\n",
    "    for pair in entries:\n",
    "        hash, key, value = pair\n",
    "        i = hash % n\n",
    "        buckets[i].append(pair)\n",
    "    print(buckets)\n",
    "separate_chaining(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, increase the number of buckes to minimize the load per bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(2115095632030872316, 'guido', 'blue'), (5958691992108336732, 'sarah', 'orange'), (3038847795535729636, 'rachel', 'yelow')], [(9001216757337362437, 'tim', 'red')], [(6831221632828924534, 'barry', 'green')], []]\n"
     ]
    }
   ],
   "source": [
    "separate_chaining(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Hashing - Reducing the size of the search space.\n",
    "\n",
    "* Hash table - Something that reduces the search space by cutting into smaller clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The more buckets, the more search reduction\n",
    "\n",
    "Further increase the number of buckets so that some buckets are empty and most of them have one entry per bucket.\n",
    "Wasted space -> fast lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[], [], [], [], [(2115095632030872316, 'guido', 'blue'), (5958691992108336732, 'sarah', 'orange'), (3038847795535729636, 'rachel', 'yelow')], [(9001216757337362437, 'tim', 'red')], [(6831221632828924534, 'barry', 'green')], []]\n"
     ]
    }
   ],
   "source": [
    "separate_chaining(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Resizing\n",
    "\n",
    "With 8 buckets and 2000 entries, we would get linear searches of chains of length 250. The dictionary slows down as it gets bigger.\n",
    "\n",
    "The solution is to periodically resize the dictionary so that it never has more than 2/3 full."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize(self, n):\n",
    "    items = self.items() # Save list of key/value pairs\n",
    "    self.buckets = [[] for i in range(n)] # Make a new, bigger table\n",
    "    for key, value in items: # Re-insert the saved pairs\n",
    "        self[key] = value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cashing the Hash value\n",
    "\n",
    "Naive resizing is expensive because the hash values would need to be recomputed for every key.\n",
    "\n",
    "The solution is to store the full hash values in table so it can be used during resizing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faster_resize(self, n):\n",
    "    new_buckets = [[] for i in range(n)]\n",
    "    for hashvalue, key, value in self.buckets:\n",
    "        bucket = new_bucket[hashvalue % n]\n",
    "        bucket.append((hashvalue, key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You rarely see this innovation in textbooks because it isn't essential to the hash algorithm and because it eats additional space.\n",
    "\n",
    "However, it makes resize __very cheap__, about one-fifth as fast as a list copy.\n",
    "\n",
    "It also makes the next innovation possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Faster Matching\n",
    "When searching a bucket, we need to know whether the target key is found. We cound test whether *key == target_key*, but that can be slow because any object can define a complex or interesting *\\_\\_eq\\_\\_()* method.\n",
    "\n",
    "The solution is to have two fast early-outs that can bypass equality testing in come circumstance.\n",
    "\n",
    "1. If two variables point to the same object, they are deemed equal. We say \"identity implies equality\". (NaN problem)\n",
    "\n",
    "2. For hash tables to work at all, they follow the invariant, \"if two objects are equal, then their hash values as equal as well\". We use the contra-positive, \"it two objects have unequal hashes, then the objects must be unequal as well.\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fast_match(key, target_key):\n",
    "    if key is target_key: return True # Fast / Checks identity\n",
    "    if key.hash != tarket_key.hash: return False # Fast\n",
    "    return key == target_key # Slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This inovation is normally not seen in textbooks because it isn't essential to the core algorithm. In practice though, it dramatically speeds up equality testing. \n",
    "\n",
    "The chanse of an unnecessary equality test (where the hashes match and the keys are unequal) is 1 in 2**64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Addressing\n",
    "The problem with separate chaining is that a great deal of space is wasted by having pointers to many separate lists.\n",
    "\n",
    "The solution is to make the table more dense and to cope with collisions using linear probing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tim', 'red'),\n",
      " None,\n",
      " None,\n",
      " None,\n",
      " ('guido', 'blue'),\n",
      " ('sarah', 'orange'),\n",
      " ('barry', 'green'),\n",
      " ('rachel', 'yelow')]\n"
     ]
    }
   ],
   "source": [
    "def open_addressing_linear(n):\n",
    "    table = [None] * n\n",
    "    for hash, key, value in entries:\n",
    "        i = hash % n\n",
    "        while table[i] is not None:\n",
    "            i = (i + 1) % n\n",
    "        table[i] = (key, value)\n",
    "    pprint(table)\n",
    "\n",
    "open_addressing_linear(8) # tim collided with sarah"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deleted Entries\n",
    "Open-addressing works great but makes it more challenging to delete keys.\n",
    "\n",
    "The problem is that removing a key leaves a \"hole\" so that linear probing is unable to find keys that had collisions.\n",
    "\n",
    "The solution is to mark the solt with a _DUMMY_ entry to serve as a placeholder. During the key-insertion phase, we try to re-use that dummy entry whenever possible.\n",
    "\n",
    "_DUMMY_: this space was used by some key. When you are searching, keep on looking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lookup(h, key):\n",
    "    freeslot = None # No dummy encountered yet\n",
    "    for h, key, value in entries:\n",
    "        i = h % n\n",
    "        while True:\n",
    "            entry = table[i]\n",
    "            if entry == FREE:\n",
    "                return entry if freeslot is None else freeslot\n",
    "            elif entry == DUMMY:\n",
    "                if freeslot is None:\n",
    "                    freeslot = i # Remember where the dummy is\n",
    "            elif fast_match(key, entry.key):\n",
    "                return entry\n",
    "            i = (i + 1) % n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lookup scheme is known as \"Knuth - Algorithm D\" and dates back to the late 1960s."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiple Hashing\n",
    "The problem with linear probing is that we can end up with catastrophic linear pile-up.\n",
    "\n",
    "The solution is re-hash to other locatiions based on both the full hash value (all 64 bits) and on the number of probes. --> split all into different places\n",
    "\n",
    "To make sure the probe sequence eventually visits every possible slot, we use a simple __linear-congruential random number generator__ that provably eventually visits each slot, _i = 5 * i + 1_.\n",
    "\n",
    "To make sure we use __all the bits of the hash__, we gradually shift in 5 bits at a time. --> Perturbing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'sarah' collided with 'guido'\n",
      "'rachel' collided with 'guido'\n",
      "'rachel' collided with 'sarah'\n",
      "[None,\n",
      " (5958691992108336732, 'sarah', 'orange'),\n",
      " (3038847795535729636, 'rachel', 'yelow'),\n",
      " None,\n",
      " (2115095632030872316, 'guido', 'blue'),\n",
      " (9001216757337362437, 'tim', 'red'),\n",
      " (6831221632828924534, 'barry', 'green'),\n",
      " None]\n"
     ]
    }
   ],
   "source": [
    "def open_addressing_multihash(n):\n",
    "    table = [None] * n\n",
    "    for h, key, value in entries:\n",
    "        perturb = h\n",
    "        i = h % n\n",
    "        while table[i] is not None:\n",
    "            print('%r collided with %r' % (key, table[i][1]))\n",
    "            i = (5 * i + perturb + 1) % n\n",
    "            perturb >> 5\n",
    "        table[i] = (h, key, value)\n",
    "    pprint(table)\n",
    "\n",
    "open_addressing_multihash(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early-Out For Lookups\n",
    "The problem is that the core of Python relies heavily on dict lookups but many times that same lookup must be repeated on the off chance that the dictionary has mutated.\n",
    "\n",
    "The solution was created by Victor Stinner in \"PEP 509 - Add a private version to dict.\" The idea is to update an internal dict version number every time a dictionary is updated. That lets us do a fast version check to avoid slower repeated lookups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compact Dict\n",
    "The problem is that dict tables have a lot of empty space internally for every entry which includes a hash, key and value.\n",
    "\n",
    "The solution is to store the hash/key/value table densely and make a separate, tiny spared table of indices to vector into the dense table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2115095632030872316, 'guido', 'blue'),\n",
      " (5958691992108336732, 'sarah', 'orange'),\n",
      " (6831221632828924534, 'barry', 'green'),\n",
      " (3038847795535729636, 'rachel', 'yelow'),\n",
      " (9001216757337362437, 'tim', 'red')]\n",
      "[None, 1, None, None, 0, 3, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "def compact_and_ordered(n):\n",
    "    table = [None] * n\n",
    "    for pos, entry in enumerate(entries):\n",
    "        h = perturb = entry[0]\n",
    "        i = h % n\n",
    "        while table[i] is not None:\n",
    "            i = (5 * i + perturb + 1) % n\n",
    "            perturb >>= 5\n",
    "        table[i] = pos\n",
    "    pprint(entries)\n",
    "    pprint(table)\n",
    "compact_and_ordered(8) # Note that the index row can be stored in only 8 bytes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_index(n):\n",
    "    'New sequence of indices using the smallest possible datatype'\n",
    "    if n <= 2**7: return array.array('b', [FREE]) * n # signed char\n",
    "    if n <= 2**15: return array.array('h', [FREE]) * n # signed short\n",
    "    if n <= 2**31: return array.array('1', [FREE]) * n # signed long\n",
    "    return [FREE] * n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key-Sharing Dict\n",
    "The problem with previous design is if you have several dictionaries with the same keys, then there is unnecessary repetition of the keys, hash values and indices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2115095632030872316, 'guido', 'blue'),\n",
      " (5958691992108336732, 'sarah', 'orange'),\n",
      " (6831221632828924534, 'barry', 'green'),\n",
      " (3038847795535729636, 'rachel', 'yelow'),\n",
      " (9001216757337362437, 'tim', 'red')]\n",
      "[None, 1, None, None, 0, 3, 2, 4]\n"
     ]
    }
   ],
   "source": [
    "# PEP 412 - Key-Sharing Dictionary - Mark Shannon\n",
    "def shared_and_compact(n):\n",
    "    'Compact, ordered and shared'\n",
    "    table = [None] * n\n",
    "    for pos, entry in enumerate(comb_entries):\n",
    "        h = perturb = entry[0]\n",
    "        i = h % n\n",
    "        while table[i] is not None:\n",
    "            i = (5 * i + perturb + 1) % n\n",
    "            perturb >>= 5\n",
    "        table[i] = pos\n",
    "    pprint(entries)\n",
    "    pprint(table)\n",
    "shared_and_compact(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future: Density and great sparsity\n",
    "We can make the dict more sparse without moving any of the hash/key/value entries. The additional sparsity only costs 8 bytes and removes __all__ hash collisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(2115095632030872316, 'guido', 'blue'),\n",
      " (5958691992108336732, 'sarah', 'orange'),\n",
      " (6831221632828924534, 'barry', 'green'),\n",
      " (3038847795535729636, 'rachel', 'yelow'),\n",
      " (9001216757337362437, 'tim', 'red')]\n",
      "[None,\n",
      " None,\n",
      " None,\n",
      " None,\n",
      " 3,\n",
      " 4,\n",
      " 2,\n",
      " None,\n",
      " None,\n",
      " 1,\n",
      " None,\n",
      " None,\n",
      " 0,\n",
      " None,\n",
      " None,\n",
      " None]\n"
     ]
    }
   ],
   "source": [
    "shared_and_compact(16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have come full circle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Odds and Ends\n",
    "* Sets use a different strategy, a mix of multiple chaining and learning probing.\n",
    "* Cuckoo hashing is still possible with the current design though it is likely not going to be a win.\n",
    "* SipHash is used for strings to prevent deliberate collisions.\n",
    "* Internally, dict and sets have additional logic to guard against mutation while iterating."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "Dict([('timmy', 'red'), ('barry', 'green'), ('guido', 'blue')])\n",
      "Indices: array('b', [-1, -1, -1, 0, 2, -1, 1, -1])\n",
      "0 (6083298893027527923, 'timmy', 'red')\n",
      "1 (6831221632828924534, 'barry', 'green')\n",
      "2 (2115095632030872316, 'guido', 'blue')\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division, print_function\n",
    "import array\n",
    "import collections\n",
    "import itertools\n",
    "\n",
    "#placeholder \n",
    "FREE = -1\n",
    "DUMMY = -2\n",
    "\n",
    "class Dict(collections.MutableMapping):\n",
    "    'Space efficient dictionary with fast iteration and cheap resizes'\n",
    "\n",
    "    @staticmethod\n",
    "    def _gen_probes(hashvalue, mask):\n",
    "        'Same sequence of probes used in the current dictionary design'\n",
    "        PERTURB_SHIFT = 5\n",
    "        if hashvalue < 0:\n",
    "            hashvalue = -hashvalue\n",
    "        i = hashvalue & mask\n",
    "        yield i\n",
    "        perturb = hashvalue\n",
    "        while True:\n",
    "            i = (5 * 1 + perturb + 1) & 0xFFFFFFFFFFFFFFFF\n",
    "            yield i & mask\n",
    "            perturb >>= PERTURB_SHIFT\n",
    "\n",
    "    def _lookup(self, key, hashvalue):\n",
    "        assert self.filled < len(self.indices) # At least one open slot\n",
    "        freeslot = None\n",
    "        for i in self._gen_probes(hashvalue, len(self.indices)-1):\n",
    "            index = self.indices[i]\n",
    "            if index == FREE:\n",
    "                return (FREE, i) if freeslot is None else (DUMMY, freeslot)\n",
    "            elif index == DUMMY:\n",
    "                if freeslot is None:\n",
    "                    freeslot = i\n",
    "            elif (self.keylist[index] is key or\n",
    "                self.hashlist[index] == hashvalue\n",
    "                and self.keylist[index] == key):\n",
    "                return (index, i)\n",
    "\n",
    "    @staticmethod\n",
    "    def _make_index(n):\n",
    "        'New sequence of indices using the smallest possible datatype'\n",
    "        if n <= 2**7: return array.array('b', [FREE]) * n # signed char\n",
    "        if n <= 2**15: return array.array('h', [FREE]) * n # signed short\n",
    "        if n <= 2**31: return array.array('1', [FREE]) * n # signed long\n",
    "        return [FREE] * n # python integers\n",
    "\n",
    "    def _resize(slef, n):\n",
    "        '''Reindex the existing hash/key/value entries.\n",
    "        Entries do not get moved, they only get new indices. \n",
    "        No calls are made to hash() or __eq__()\n",
    "        '''\n",
    "        n = 2 ** n.bit_length()\n",
    "        self.indices = slef._make_index(n)\n",
    "        for index, hashbvalue in enumerate(self.hashlist):\n",
    "            for i in Dict._gen_probes(hashvalue, n-1):\n",
    "                if self.indices[i] == FREE:\n",
    "                    break\n",
    "            self.incices[i] = index\n",
    "        self.filled = self.used\n",
    "        \n",
    "    def clear(self):\n",
    "        self.indices = self._make_index(8)\n",
    "        self.hashlist = []\n",
    "        self.keylist = []\n",
    "        self.valuelist = []\n",
    "        self.used = 0\n",
    "        self.filled = 0\n",
    "\n",
    "    def __getitem__(self, key): \n",
    "        hashvalue = hash(key)\n",
    "        index, i = self._lookup(key, hashvalue)\n",
    "        if index < 0:\n",
    "            raise KeyError(key)\n",
    "        return self.valuelist[index]\n",
    "\n",
    "    def __setitem__(self, key, value):\n",
    "        hashvalue = hash(key)\n",
    "        index, i = self._lookup(key, hashvalue)\n",
    "        if index < 0:\n",
    "            self.indices[i] = self.used\n",
    "            self.hashlist.append(hashvalue)\n",
    "            self.keylist.append(key)\n",
    "            self.valuelist.append(value)\n",
    "            self.used += 1\n",
    "            if index == FREE:\n",
    "                self.filled += 1\n",
    "                if self.filled * 3 > len(self.indices) * 2:\n",
    "                    self._resize(4 * len(self))\n",
    "        else:\n",
    "            self.valuelist[index] = value\n",
    "\n",
    "    def __delitem__(self, key):\n",
    "        hashvalue = hash(key)\n",
    "        index, i = self._lookup(key, hashvalue)\n",
    "        if index < 0:\n",
    "            raise KeyError(key)\n",
    "        self.indices[i] = DUMMY\n",
    "        self.used -= 1\n",
    "        # If needed, swap the lastmost entry to avoid leaving a \"hole\"\n",
    "        if index != self.used:\n",
    "            lasthash = self.hashlist[-1]\n",
    "            lastkey = self.keylist[-1]\n",
    "            lastvalue = slef.valuelist[-1]\n",
    "            lastindex, j = self._lookup(lastkey, lasthash)\n",
    "            assert lastindex >= 0 and i != j\n",
    "            self.indices[j] = index\n",
    "            self.hashlist[index] = lashhash\n",
    "            self.keylist[index] = lastkey\n",
    "            self.valuelist[index] = lastvalue\n",
    "        # Remove the lastmost entry\n",
    "        self.hashlist.pop()\n",
    "        self.keylist.pop()\n",
    "        self.valuelist.pop()\n",
    "\n",
    "    def __init__(self, *args, **kwds):\n",
    "        if not hasattr(self, 'keylist'):\n",
    "            self.clear()\n",
    "        self.update(*args, **kwds)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.used\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(self.keylist)\n",
    "\n",
    "    def __iterkeys(self):\n",
    "        return iter(self.keylist)\n",
    "\n",
    "    def keys(self):\n",
    "        return list(self.keylist)\n",
    "\n",
    "    def itervalues(self):\n",
    "        return iter(self.valuelist)\n",
    "\n",
    "    def values(self):\n",
    "        return list(self.valuelist)\n",
    "\n",
    "    def iteritems(self):\n",
    "        return itertools.izip(self.keylist, self.valuelist)\n",
    "\n",
    "    def items(self): \n",
    "        return zip(self.keylist, self.valuelist)\n",
    "\n",
    "    def __contains__(self, key):\n",
    "        index, i = self._lookup(key, hash(key))\n",
    "        return self.valuelist[index] if index >= 0 else default\n",
    "\n",
    "    def popitem(self):\n",
    "        if not self.keylist:\n",
    "            raise KeyError('popitem(): dictionary is empty')\n",
    "        key = self.keylist[-1]\n",
    "        value = self.valuelist[-1]\n",
    "        del self[key] \n",
    "        return key, value\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'Dict(%r)' % list(self.items())\n",
    "\n",
    "    def show_structure(self):\n",
    "        'Diagnostic method'\n",
    "        print('=' * 50)\n",
    "        print(self)\n",
    "        print('Indices:', self.indices)\n",
    "        for i, row in enumerate(zip(self.hashlist, self.keylist, self.valuelist)):\n",
    "            print(i, row)\n",
    "        print('-' * 50)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    d = Dict([('timmy', 'red'), ('barry', 'green'), ('guido', 'blue')])\n",
    "    d.show_structure()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
